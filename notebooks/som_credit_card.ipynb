{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Credit Card Approvals\n",
    "\n",
    "## The Scene\n",
    "You are a Data Scientist at a bank. You have a historical dataset of credit card applications. Some were **Approved (+)**, others were **Rejected (-)**.\n",
    "\n",
    "Your boss asks: *\"Can we visualize our customer base? Are the approved customers distinct from the rejected ones, or is it all mixed up?\"*\n",
    "\n",
    "Since the data is high-dimensional (Age, Debt, BankCustomer, Employment, etc.), you can't just plot `x` vs `y`. You need a way to map this high-dimensional data onto a 2D map.\n",
    "\n",
    "**Enter the Self-Organizing Map (SOM).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Data Pipeline\n",
    "Real-world data is messy. We are using the **UCI Credit Approval Dataset**.\n",
    "- **Confidential**: All attribute names and values have been changed to meaningless symbols to protect user privacy.\n",
    "- **Noisy**: It contains missing values (`?`) and a mix of numbers and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and Load Data\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\"\n",
    "dataset = pd.read_csv(url, header=None)\n",
    "\n",
    "# De-anonymize columns for educational clarity\n",
    "# Mapping based on common UCI dataset interpretations\n",
    "columns = [\"Gender\", \"Age\", \"Debt\", \"Married\", \"BankCustomer\", \"EducationLevel\", \"Ethnicity\", \"YearsEmployed\", \"PriorDefault\", \"Employed\", \"CreditScore\", \"DriversLicense\", \"Citizen\", \"ZipCode\", \"Income\", \"Approved\"]\n",
    "dataset.columns = columns\n",
    "\n",
    "# Peek at the mess\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "1.  **Missing Values**: The dataset uses `?` for missing info. We replace them with `NaN` and then impute (fill) them.\n",
    "    - **Numerical Columns**: Fill with the **Mean**.\n",
    "    - **Categorical Columns**: Fill with the **Mode** (Most frequent value).\n",
    "2.  **Encoding**: Computers need numbers. We use `LabelEncoder` to turn text categories (e.g., 'u', 'y', 'l') into numbers (0, 1, 2...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Handle '?'\n",
    "dataset = dataset.replace('?', np.nan)\n",
    "\n",
    "# Data Type Correction: Ensure numeric columns are actually numeric\n",
    "for col in dataset.columns:\n",
    "    # Try to convert to numeric, but keep as object if it fails (safe replacement for errors='ignore')\\n\n",
    "    try:\\n\n",
    "        dataset[col] = pd.to_numeric(dataset[col])\\n\n",
    "    except (ValueError, TypeError):\\n\n",
    "        pass\\n\n",
    "\n",
    "# 2. Impute Missing Values\n",
    "for col in dataset.columns:\n",
    "    if dataset[col].dtype == 'object':\n",
    "        dataset[col] = dataset[col].fillna(dataset[col].mode()[0])\n",
    "    else:\n",
    "        # Use mean for numerical columns\n",
    "        dataset[col] = dataset[col].fillna(dataset[col].mean())\n",
    "\n",
    "# 3. Check for any remaining nulls\n",
    "print(\"Remaining nulls:\", dataset.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Encode Categorical Data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in dataset.columns:\n",
    "    if dataset[col].dtype == 'object':\n",
    "        dataset[col] = le.fit_transform(dataset[col])\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction & Scaling\n",
    "- **X**: All columns except the last one (Attributes).\n",
    "- **y**: The last column (Approved/Rejected).\n",
    "- **Scaling**: SOMs use distance (Euclidean). We strictly scale everything to 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values\n",
    "\n",
    "# Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range=(0, 1))\n",
    "X = sc.fit_transform(X)\n",
    "print(f\"Data Scaled. X shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Model (SOM)\n",
    "We reuse our robust `SimpleSOM` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSOM:\n",
    "    def __init__(self, x, y, input_len):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.input_len = input_len\n",
    "        self.weights = np.random.random((x, y, input_len))\n",
    "        self.learning_rate = 0.5\n",
    "        self.radius = max(x, y) / 2\n",
    "        self.time_constant = 1.0 \n",
    "\n",
    "    def find_winner(self, sample):\n",
    "        diff = self.weights - sample\n",
    "        sq_dist = np.sum(diff**2, axis=2)\n",
    "        return np.unravel_index(np.argmin(sq_dist), (self.x, self.y))\n",
    "\n",
    "    def update_weights(self, sample, winner, iteration):\n",
    "        rad = self.radius * np.exp(-iteration / self.time_constant)\n",
    "        lr = self.learning_rate * np.exp(-iteration / self.time_constant)\n",
    "        \n",
    "        if rad < 1e-10: rad = 1e-10\n",
    "\n",
    "        for i in range(self.x):\n",
    "            for j in range(self.y):\n",
    "                dist = np.sqrt((i - winner[0])**2 + (j - winner[1])**2)\n",
    "                if dist <= rad:\n",
    "                    influence = np.exp(-(dist**2) / (2 * (rad**2)))\n",
    "                    self.weights[i, j] += lr * influence * (sample - self.weights[i, j])\n",
    "\n",
    "    def train(self, data, num_epochs):\n",
    "        total_steps = len(data) * num_epochs\n",
    "        self.time_constant = total_steps / np.log(self.radius)\n",
    "        \n",
    "        step = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            np.random.shuffle(data)\n",
    "            for sample in data:\n",
    "                winner = self.find_winner(sample)\n",
    "                self.update_weights(sample, winner, step)\n",
    "                step += 1\n",
    "\n",
    "    def distance_map(self):\n",
    "        dmap = np.zeros((self.x, self.y))\n",
    "        for i in range(self.x):\n",
    "            for j in range(self.y):\n",
    "                neighbors = []\n",
    "                if i > 0: neighbors.append(self.weights[i-1, j])\n",
    "                if i < self.x-1: neighbors.append(self.weights[i+1, j])\n",
    "                if j > 0: neighbors.append(self.weights[i, j-1])\n",
    "                if j < self.y-1: neighbors.append(self.weights[i, j+1])\n",
    "                if len(neighbors) > 0:\n",
    "                    dists = [np.linalg.norm(self.weights[i, j] - n) for n in neighbors]\n",
    "                    dmap[i, j] = np.mean(dists)\n",
    "        return (dmap - dmap.min()) / (dmap.max() - dmap.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "We use a **10x10 Grid**. This provides 100 distinctive \"Customer Archetypes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = SimpleSOM(10, 10, X.shape[1])\n",
    "som.train(X, num_epochs=100)\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization: Approved vs Rejected\n",
    "We map the customers back to the grid.\n",
    "- **Green Squares**: Approved Customers (+)\n",
    "- **Red Circles**: Rejected Customers (-)\n",
    "\n",
    "**Hypothesis**: If the machine learning works, the Green Squares and Red Circles should occupy different territories on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# 1. Distance Map (Background)\n",
    "plt.imshow(som.distance_map(), cmap='bone', interpolation='none')\n",
    "plt.colorbar(label='Mean Inter-neuron Distance')\n",
    "\n",
    "# 2. Plotting Markers\n",
    "# We handle the mapping explicitly because LabelEncoder might assign 0/1 arbitrarily\n",
    "try:\n",
    "    pos_code = le.transform(['+'])[0]\n",
    "except:\n",
    "    pos_code = 0 # Fallback if '+' not found (unlikely)\n",
    "\n",
    "for i, x in enumerate(X):\n",
    "    w = som.find_winner(x)\n",
    "    \n",
    "    if y[i] == pos_code:\n",
    "        # Approved -> Green Square\n",
    "        m = 's'\n",
    "        c = 'g'\n",
    "    else:\n",
    "        # Rejected -> Red Circle\n",
    "        m = 'o'\n",
    "        c = 'r'\n",
    "    \n",
    "    plt.plot(w[1], w[0],\n",
    "             m,\n",
    "             markeredgecolor=c,\n",
    "             markerfacecolor='None',\n",
    "             markersize=10,\n",
    "             markeredgewidth=2)\n",
    "\n",
    "plt.title('Credit Card Applications\\n(Green=Approved, Red=Rejected)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Feature Analysis\n",
    "To understand **why** certain areas are approved or rejected, we can visualize the distribution of specific features across the map. \n",
    "\n",
    "Select a feature below (e.g., `Income`, `Debt`, `YearsEmployed`) to see where high values are clustered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "def plot_feature_map(feature_name):\n",
    "    # Find the column index in X\n",
    "    # Note: dataset has 'Approved' at the end, X does not include it.\n",
    "    feature_idx = list(dataset.columns[:-1]).index(feature_name)\n",
    "    \n",
    "    # Calculate average feature value for each neuron\n",
    "    feature_map = np.zeros((som.x, som.y))\n",
    "    counts = np.zeros((som.x, som.y))\n",
    "    \n",
    "    for i, x in enumerate(X):\n",
    "        w = som.find_winner(x)\n",
    "        # The value is scaled (0-1), which is good for visualization consistency\n",
    "        val = X[i, feature_idx] \n",
    "        feature_map[w] += val\n",
    "        counts[w] += 1\n",
    "        \n",
    "    # Avoid division by zero\n",
    "    feature_map = np.divide(feature_map, counts, out=np.zeros_like(feature_map), where=counts!=0)\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.title(f\"SOM Heatmap: {feature_name}\\n(Yellow=High, Purple=Low)\")\n",
    "    plt.imshow(feature_map, cmap='viridis', interpolation='none')\n",
    "    plt.colorbar(label='Feature Value (Scaled 0-1)')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Explore what defines each cluster:\")\n",
    "interact(plot_feature_map, feature_name=dataset.columns[:-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. \"Will I Get Approved?\" Simulator\n",
    "Test your luck! Enter numerical details for a hypothetical applicant. \n",
    "- We will map this new person onto the grid.\n",
    "- **Gold Star**: Your position.\n",
    "- If you land in a \"Green Square\" region, you are likely **Approved**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rate a New Applicant:\")\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "# Widgets for key numerical features\n",
    "# Note: Ranges are approximate based on dataset analysis\n",
    "w_age = widgets.FloatSlider(value=30.0, min=15.0, max=80.0, step=1.0, description='Age:', style=style)\n",
    "w_debt = widgets.FloatSlider(value=5.0, min=0.0, max=30.0, step=0.1, description='Debt:', style=style)\n",
    "w_years = widgets.FloatSlider(value=2.0, min=0.0, max=20.0, step=0.1, description='Years Emp:', style=style)\n",
    "w_income = widgets.FloatSlider(value=1000.0, min=0.0, max=10000.0, step=100.0, description='Income:', style=style)\n",
    "w_score = widgets.FloatSlider(value=1.0, min=0.0, max=20.0, step=1.0, description='Credit Score:', style=style)\n",
    "\n",
    "btn = widgets.Button(description=\"Map Me!\", button_style='success')\n",
    "out = widgets.Output()\n",
    "\n",
    "def map_new_applicant(b):\n",
    "    with out:\n",
    "        out.clear_output()\n",
    "        \n",
    "        # 1. Create a baseline vector (average applicant)\n",
    "        # Since X is already scaled, we take the mean of the SCALED data\n",
    "        avg_applicant_scaled = np.mean(X, axis=0)\n",
    "        \n",
    "        # 2. Unscale it to get original units (approx) so we can modify it with Slider values\n",
    "        # shape needed for inverse_transform is (1, 15)\n",
    "        avg_applicant_original = sc.inverse_transform([avg_applicant_scaled])[0]\n",
    "        \n",
    "        # 3. Update with User Input\n",
    "        # We need to know indices: Age(1), Debt(2), YearsEmployed(7), CreditScore(10), Income(14)\n",
    "        # These mappings correspond to our 'columns' list defined earlier.\n",
    "        \n",
    "        new_data = avg_applicant_original.copy()\n",
    "        new_data[1] = w_age.value       # Age\n",
    "        new_data[2] = w_debt.value      # Debt\n",
    "        new_data[7] = w_years.value     # YearsEmployed\n",
    "        new_data[10] = w_score.value    # CreditScore\n",
    "        new_data[14] = w_income.value   # Income\n",
    "\n",
    "        # 4. Scale it back\n",
    "        final_vector_scaled = sc.transform([new_data])[0]\n",
    "        \n",
    "        # 5. Find Winner\n",
    "        winner = som.find_winner(final_vector_scaled)\n",
    "        \n",
    "        # 6. Plot\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        # Background\n",
    "        plt.imshow(som.distance_map(), cmap='bone', interpolation='none')\n",
    "        \n",
    "        # Plot existing approved/rejected (lighter alpha to emphasize the new one)\n",
    "        for i, x in enumerate(X):\n",
    "            w = som.find_winner(x)\n",
    "            if y[i] == pos_code: # Approved\n",
    "                plt.plot(w[1], w[0], 's', markeredgecolor='g', markerfacecolor='None', alpha=0.1)\n",
    "            else: # Rejected\n",
    "                plt.plot(w[1], w[0], 'o', markeredgecolor='r', markerfacecolor='None', alpha=0.1)\n",
    "        \n",
    "        # Plot YOU (Gold Star)\n",
    "        plt.plot(winner[1], winner[0], '*', markeredgecolor='gold', markerfacecolor='gold', markersize=20, label='YOU')\n",
    "        plt.legend()\n",
    "        plt.title(\"Your Position on the Credit Map\")\n",
    "        plt.show()\n",
    "\n",
    "btn.on_click(map_new_applicant)\n",
    "\n",
    "display(w_age, w_debt, w_years, w_score, w_income, btn, out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "introai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
